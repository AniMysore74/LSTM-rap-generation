{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lyric generation with LSTMs\n",
    "\n",
    "**Author : ** Aniruddha Mysore\n",
    "\n",
    "Lyric data has been parsed from Lyrics Wikia. The songlist was parsed manually with beautiful soup, and used the API to get lyrics of each song - [API](https://github.com/rhnvrm/lyric-api)\n",
    "\n",
    "**Credits: **\n",
    " \n",
    "1. Videos on LSTMs and RNNs by Siraj Raval (Youtube)\n",
    "\n",
    "2. Ivan Liljeqvist's [article](https://medium.com/@ivanliljeqvist/using-ai-to-generate-lyrics-5aba7950903) on using Keras for generating lyrics and his [code](https://github.com/ivan-liljeqvist/ailyrics/) \n",
    "\n",
    "**Disclaimer:** These are [Eminem's](https://www.google.com/search?q=eminem) lyrics, so the predictor will learn and use th. Cuss in, Cuss out.\n",
    "\n",
    "![](https://data.whicdn.com/images/36141347/large.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First: Data Collection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 songs had broken links, skipping.\n",
      "Explosion\n",
      "Real 911\n",
      "Don't Call Me Marshall\n",
      "Shady Camp\n",
      "Victory\n",
      "Back Down Royce\n",
      "The Boston Bitch\n",
      "Nail in the Coffin\n",
      "Slut Phone Call\n",
      "Parking Lot Flows\n",
      "8 More Miles\n",
      "The Cross\n",
      "Many Men (DJ Green Lantern Remix)\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from urllib.request import urlopen\n",
    "import urllib.request\n",
    "import re \n",
    "\n",
    "# Get html\n",
    "url = 'http://lyrics.wikia.com/wiki/Eminem'\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.text, 'lxml')\n",
    "\n",
    "count = 0\n",
    "data = []\n",
    "broken = []\n",
    "# Parse the data to get list of songs and urls\n",
    "for album in soup.find_all(class_='album-art'):\n",
    "    count += 1\n",
    "    for song in album.find_next('ol').children:\n",
    "        try:\n",
    "            a = re.search('\\:(.*)', song.b.a['href'])\n",
    "            data.append({\n",
    "                'url': song.b.a['href'],\n",
    "                'name': a.group(1)\n",
    "            })\n",
    "        except:\n",
    "            broken.append(song.b.a.text)\n",
    "\n",
    "if broken:\n",
    "    print(f'{len(broken)} songs had broken links, skipping.')\n",
    "    [print(song) for song in broken]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our dataset size: 305 songs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/wiki/Eminem:Infinite</td>\n",
       "      <td>Infinite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/wiki/Eminem:W.E.G.O.</td>\n",
       "      <td>W.E.G.O.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/wiki/Eminem:It%27s_OK</td>\n",
       "      <td>It%27s_OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/wiki/Eminem:313</td>\n",
       "      <td>313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/wiki/Eminem:Tonite</td>\n",
       "      <td>Tonite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/wiki/Eminem:Maxine</td>\n",
       "      <td>Maxine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/wiki/Eminem:Open_Mic</td>\n",
       "      <td>Open_Mic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/wiki/Eminem:Never_2_Far</td>\n",
       "      <td>Never_2_Far</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/wiki/Eminem:Searchin%27</td>\n",
       "      <td>Searchin%27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>/wiki/Eminem:Backstabber</td>\n",
       "      <td>Backstabber</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>/wiki/Eminem:Jealousy_Woes_II</td>\n",
       "      <td>Jealousy_Woes_II</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>/wiki/Eminem:Intro_(Slim_Shady)</td>\n",
       "      <td>Intro_(Slim_Shady)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>/wiki/Eminem:Low_Down_Dirty</td>\n",
       "      <td>Low_Down_Dirty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>/wiki/Eminem:If_I_Had</td>\n",
       "      <td>If_I_Had</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>/wiki/Eminem:Just_Don%27t_Give_A_Fuck</td>\n",
       "      <td>Just_Don%27t_Give_A_Fuck</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      url                      name\n",
       "0                   /wiki/Eminem:Infinite                  Infinite\n",
       "1                   /wiki/Eminem:W.E.G.O.                  W.E.G.O.\n",
       "2                  /wiki/Eminem:It%27s_OK                 It%27s_OK\n",
       "3                        /wiki/Eminem:313                       313\n",
       "4                     /wiki/Eminem:Tonite                    Tonite\n",
       "5                     /wiki/Eminem:Maxine                    Maxine\n",
       "6                   /wiki/Eminem:Open_Mic                  Open_Mic\n",
       "7                /wiki/Eminem:Never_2_Far               Never_2_Far\n",
       "8                /wiki/Eminem:Searchin%27               Searchin%27\n",
       "9                /wiki/Eminem:Backstabber               Backstabber\n",
       "10          /wiki/Eminem:Jealousy_Woes_II          Jealousy_Woes_II\n",
       "11        /wiki/Eminem:Intro_(Slim_Shady)        Intro_(Slim_Shady)\n",
       "12            /wiki/Eminem:Low_Down_Dirty            Low_Down_Dirty\n",
       "13                  /wiki/Eminem:If_I_Had                  If_I_Had\n",
       "14  /wiki/Eminem:Just_Don%27t_Give_A_Fuck  Just_Don%27t_Give_A_Fuck"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data, columns=[\"url\",\"name\"])\n",
    "print(f'Our dataset size: {df.shape[0]} songs')\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr. Dre:\n",
      "Y'all know me, still the same ol' G, but I been low-key\n",
      "Hated on by most these niggas with no cheese, no deals and no G's\n",
      "No wheels and no keys, no boats no snowmobiles and no skis\n",
      "Mad at me 'cause I can finally afford to provide my family with groceries\n",
      "\n",
      "Got a crib with a studio and it's all full o' tracks, to add to the wall full o' plaques \n",
      "Hangin' up in the office in back of my house like trophies\n",
      "Did y'all think I'ma let my dough freeze, ho please\n",
      "You better bow down on both knees,...\n"
     ]
    }
   ],
   "source": [
    "# here's what the lyrics look like\n",
    "data = json.load(urlopen('http://lyric-api.herokuapp.com/api/find/Eminem/'+df.iloc[80]['name']))\n",
    "print(data['lyric'][:500]+'...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Infinite\n",
      "20 Just_Don%27t_Give_A_Fuck\n",
      "40 Still_Don%27t_Give_A_Fuck\n",
      "60 Under_The_Influence\n",
      "80 Forgot_About_Dre\n",
      "100 The_Kiss\n",
      "120 Come_On_In\n",
      "140 Renegade\n",
      "160 Puke\n",
      "ERROR : 173 Encore_/_Curtains_Down\n",
      "180 Freestyle_(Dissin%27_The_Source)\n",
      "200 Stan_(Live)\n",
      "220 Underground\n",
      "240 No_Love\n",
      "260 Brainless\n",
      "280 Revival_(Interlude)\n",
      "300 Fall\n"
     ]
    }
   ],
   "source": [
    "#Saving the corpus file for training the model\n",
    "\n",
    "corpus = \"\"\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    try:\n",
    "        data = json.load(urlopen('http://lyric-api.herokuapp.com/api/find/Eminem/'+row['name']))\n",
    "        corpus += \"\\n\" + data['lyric']\n",
    "    except urllib.error.HTTPError:\n",
    "        print(\"ERROR :\",index, row['name'])\n",
    "    else:\n",
    "        # Print title of every 20th song that we fetch\n",
    "        if index%20 ==0:\n",
    "            print(index, row['name'])\n",
    "\n",
    "#UNCOMMENT THIS TO OVERWRITE EXITING CORPUS\n",
    "#You may need to train the model again if you do this\n",
    "\n",
    "#with open(\"corpus.txt\", \"w\") as text_file:\n",
    "#    text_file.write(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our corpus file saved, it's time for\n",
    "\n",
    "## Second: Training the Model\n",
    "\n",
    "Before training we define the length of each line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "PATH = \"corpus.txt\" \n",
    "sequence_length = 40\n",
    "step = 3\n",
    "\n",
    "\n",
    "text = []\n",
    "chars = []\n",
    "\n",
    "\n",
    "# get the lyrics corpus from the file\n",
    "with io.open(PATH, 'r', encoding='utf8') as f:\n",
    "    text = f.read().lower()\n",
    "    chars = sorted(list(set(text)))\n",
    "\n",
    "# sequences is input to nueral network\n",
    "# next_chars are labels while training\n",
    "sequences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - sequence_length, step):\n",
    "    sequences.append(text[i: i + sequence_length])\n",
    "    next_chars.append(text[i + sequence_length])\n",
    "\n",
    "    \n",
    "char_to_index = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "\n",
    "We need to convert all our character strings into a format that can be used by the LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# vectorize the data since we cannot use characters and strings \n",
    "\n",
    "X = np.zeros((len(sequences), sequence_length, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sequences), len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, sentence in enumerate(sequences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_to_index[char]] = 1\n",
    "        y[i, char_to_index[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "This may take some time to run. Be default the model trains for 20 epochs.\n",
    "\n",
    "On my laptop (Intel i5, 7th Gen, NVIDIA 940MX) , each epoch takes about 2 minutes 30 seconds\n",
    "\n",
    "You can skip the training by using the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aniruddha/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/aniruddha/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:493: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/aniruddha/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:494: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/aniruddha/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:495: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/aniruddha/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:496: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/aniruddha/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:497: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/aniruddha/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:502: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 128)               96256     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 59)                7611      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 59)                0         \n",
      "=================================================================\n",
      "Total params: 103,867\n",
      "Trainable params: 103,867\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# MODEL TRAINING\n",
    "# skip this if you want to use the pretrained model\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "EPOCHS = 20\n",
    "\n",
    "# this is our keras model. It has 128 LSTM neurons\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(sequence_length, len(chars))))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNCOMMENT TO TRAIN : \n",
    "\n",
    "#model.fit(X, y, batch_size=128, nb_epoch=EPOCHS)\n",
    "#model.save('eminem.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# load pretrained\n",
    "\n",
    "model = load_model(\"eminem.h5\")  # you can skip training by loading the trained weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third: Predictions\n",
    "\n",
    "Now for the fun part :)\n",
    "\n",
    "The diversity parameter controls how similar each line of lyrics will be. The iteration explores lyrics at different values of Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================\n",
      "DIVERSITY: 0.2\n",
      "SEED: \"my favourite food is peanut butter and j\"\n",
      "====================================================\n",
      "my favourite food is peanut butter and just to the show\n",
      "i got the change to the party shit i was a bring in my shove and beef the man and the shit the shit i was a motherfuckin' the more than you say i was so really started to say\n",
      "there is it and the shit i was a show a money\n",
      "the morning and i can say i got the shots in the morning and i can say i was some same should get a missin'\n",
      "the shit i am a man, so the shit i got the but i think \n",
      "\n",
      "====================================================\n",
      "DIVERSITY: 0.5\n",
      "SEED: \"my favourite food is peanut butter and j\"\n",
      "====================================================\n",
      "my favourite food is peanut butter and just only shake my hain\n",
      "\n",
      "it's soon, i hate your shit i am a sing of the ass\n",
      "\n",
      "the weed suffed now my dame of a motherfucker\n",
      "you got a controlly back with the lead, the shot\n",
      "i was the pass to the means\n",
      "\n",
      "i'm still the head to the bomb lins\n",
      "\n",
      "i'm so you're a time i'm gonna started with a said for the bottom and party\n",
      "i don't get your hotes and high and the shit\n",
      "\n",
      "who are straight on your fare\n",
      "\n",
      "guess i am\n",
      "\n",
      "====================================================\n",
      "DIVERSITY: 1.0\n",
      "SEED: \"my favourite food is peanut butter and j\"\n",
      "====================================================\n",
      "my favourite food is peanut butter and jobs your bitch \n",
      "still say you say it's a pretch, jidebosing\n",
      "and these for more yates and goin' on side\n",
      "and this own for teg to a rong bitch\n",
      "\n",
      "i had a real to be cracks laughs his eugh\n",
      "but i don't heard. money afr you want your ecretsing your truck\n",
      "anyon' so lets evel gro? i be eats 'em only be auta\n",
      "\n",
      ", lonely a tamiraliction i do\n",
      "i hatepen!\n",
      "he ever mosesvers as a lookin' on no, kan\n",
      "\n",
      "y'ar bad san\n",
      "all\n",
      "\n",
      "====================================================\n",
      "DIVERSITY: 1.2\n",
      "SEED: \"my favourite food is peanut butter and j\"\n",
      "====================================================\n",
      "my favourite food is peanut butter and just hyain\n",
      "mighty\n",
      "and tired of bocknomabler, what ie so even done\n",
      "uny'atl had, crashelacllisure a chills\n",
      "to get your ladiess takion, and back a faithippele\n",
      "he have alday pycwarmation lot big gr, on bob back\n",
      "any call, as nistict's chroilyppers insealt ssfache over high this megr\n",
      "the freants hogme? ...r. tendio to drun a that's kiss me to come oc foreqialing\n",
      "oer prijuped wradcutlege for my lake she's\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "INPUT = \"My favourite food is peanut butter and j\"\n",
    "\n",
    "if len(INPUT) is not 40:\n",
    "    print(\"Sentence length needs to be 40. It currently is\", len(INPUT))\n",
    "\n",
    "else:\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print()\n",
    "        print('====================================================\\nDIVERSITY:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        # insert your 40-chars long string. OBS it needs to be exactly 40 chars!\n",
    "        sentence = INPUT\n",
    "        sentence = sentence.lower()\n",
    "        generated += sentence\n",
    "\n",
    "        print('SEED: \"' + sentence + '\"\\n====================================================')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x = np.zeros((1, sequence_length, len(chars)))\n",
    "\n",
    "            for t, char in enumerate(sentence):\n",
    "                x[0, t, char_to_index[char]] = 1.\n",
    "                \n",
    "            predictions = model.predict(x, verbose=0)[0]\n",
    "\n",
    "            if diversity == 0:\n",
    "                diversity = 1\n",
    "\n",
    "            preds = np.asarray(predictions).astype('float64')\n",
    "            preds = np.log(preds) / diversity\n",
    "            exp_preds = np.exp(preds)\n",
    "            preds = exp_preds / np.sum(exp_preds)\n",
    "            probas = np.random.multinomial(1, preds, 1)\n",
    "            next_index =  np.argmax(probas)\n",
    "\n",
    "\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://vignette.wikia.nocookie.net/looneytunes/images/e/e1/All.jpg/revision/latest/scale-to-width-down/260?cb=20150313020828)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
